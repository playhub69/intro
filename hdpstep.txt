AIM - Install Hadoop in Stand-Alone Mode on Ubuntu

sudo apt update
java -version

Create a new user- urname
sudo adduser urname

/home/urname    --- check in file explorer urname folder should be created
ctrl + L to select path of file explorer
cd /home/urname    --- use this on terminal to check 


Provide privileges
sudo nano /etc/sudoers ------------ to open file for editing sudo nano is used
urname ALL=(ALL:ALL) ALL ------ add this line in file

to save
ctrls + x
press Y

to switch user
su - urname  

to set up key-based ssh
ssh -keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys

ssh localhost

copy the existing setup from source to destination /home/urname/
sudo mv /home/downloads/hadoop-3.2.0.tar.gz /home/urname

cd /home/urname     ---- go to ur user folder
tar xzvf hadoop-3.2.0.tar.gz    ---- extract hadoop tar file

after extraction hadoop-3.2.0 folder will get created inside urname

sudo nano .bashrc

add all below export wala lines at the botton of the bashrc file

export JAVA_HOME=/lib/jvm/java-11-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin
export HADOOP_HOME=/home/urname/hadoop-3.2.0
export HADOOP_COMMON_HOME=/home/urname/hadoop-3.2.0
export HADOOP_MAPRED_HOME=/home/urname/hadoop-3.2.0
export PATH=$PATH:$HADOOP_COMMON_HOME/bin
export PATH=$PATH:$HADOOP_COMMON_HOME/sbin

save file then Run below command if it comes on next cmd prompt then file editing is proper
source ~/.bashrc
urname
Checking of java and hadoop
java -version
hadoop version

sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh

Add the following line at the end of hadoop-env.sh file
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/

to save the changes in the file, press ctrl and x together.
then press Y
then press Enter key

$HADOOP_HOME/bin/hadoop

__________________________________________________________________________________________
AIM- Install Hadoop in Pseudo Distributed Mode on Ubuntu

all steps same as above till extraction 

then edit bachrc file -- need to add below new parameters 

below parameters for prac1 only
export JAVA_HOME=/lib/jvm/java-11-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin
export HADOOP_HOME=/home/urname/hadoop-3.2.0
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME= $HADOOP_HOME
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_COMMON_HOME/bin
export PATH=$PATH:$HADOOP_COMMON_HOME/sbin

open hadoop-env.sh file 
sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh 

Add the following line at the end of .sh file
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
=================================================================================
3 core-site.xml
=================================================================================
sudo nano /home/urname/hadoop-3.2.0/etc/hadoop/core-site.xml

<property>
	<name>hadoop.tmp.dir</name>
	<value>/home/hadoop/tmpdata</value>
	<description>A base for other temporarary directories</description>
</property>

<property>
	<name>fs.default.name</name>
	<value>hdfs://localhost:9000</value>
	<description>The name of the default file system.</description>
</property>

=================================================================================
4 hdfs-site.xml
=================================================================================
sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
<property>
	<name>dfs.data.dir</name>
	<value>/home/hadoop/dfsdata/namenode</value>
	<description>Location of namenode</description>
</property>

<property>
	<name>dfs.data.dir</name>
	<value>/home/hadoop/dfsdata/datanode</value>
	<description>Location of datanode</description>
</property>

<property>
	<name>dfs.replication</name>
	<value>1</value>
	<description>Replication Factor</description>
</property>

=================================================================================
5 mapred-site.xml
=================================================================================
sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
<property>
	<name>mapreduce.framework.name</name>
	<value>yarn</value>
	<description>Name of my mapreduce framework</description>
</property>


=================================================================================
6 yarn-site.xml  -- will be given

Format Namenode
=================================================================================
move to the /bin folder
cd /home/urname/hadoop-3.2.0/bin

Now format the namenode using the following command
hdfs namenode -format

=================================================================================
Start the namenode, datanode
=================================================================================
move to the /sbin folder

cd /home/urname/hadoop-3.2.0/sbin
start-dfs.sh
start-yarn.sh


=================================================================================
Start the task tracker and job tracker // Skip in case file not present
=================================================================================
start-mapred.sh   -- won't be runnable


=================================================================================
To check if Hadoop started correctly
=================================================================================
jps


=================================================================================
Copy file from local system to hdfs
=================================================================================
hdfs dfs -put source    -- source will be path of any text file on ubuntu vm that u will be copying

check using below listing command on terminal
hdfs dfs -ls


=================================================================================
Check with browser
=================================================================================
DFS overview - 
http://localhost:9870/ 
Datanode     - 
http://localhost:9864/
