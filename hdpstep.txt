AIM - Install Hadoop in Stand-Alone Mode on Ubuntu

sudo apt update
java -version

to Create a new user- ernest

sudo adduser ernest

/home/ernest --- check in file explorer Ernest folder should be created
ctrl + L to select path of file explorer
cd /home/ernest --- use this on terminal to check 

to Provide privileges
sudo nano /etc/sudoers ------------ to open file for editing sudo nano is used
ernest ALL=(ALL:ALL) ALL ------ add this line in file

to save
ctrls + x
press Y

to switch user
su - ernest  
sudo apt update
java â€“version

to set up key-based ssh
ssh -keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
ssh localhost

to copy the existing setup from source to destination /home/ernest/
sudo mv /home/downloads/hadoop-3.2.0.tar.gz /home/ernest

cd /home/ernest      ---- go to ur user folder
tar xzvf hadoop-3.2.0.tar.gz    ---- extract hadoop tar file

after extraction hadoop-3.2.0 folder will get created inside ernest

sudo nano .bashrc

add all below export wala lines at the botton of the bashrc file

export JAVA_HOME=/lib/jvm/java-11-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin
export HADOOP_HOME=/home/ernest/hadoop-3.2.0
export HADOOP_COMMON_HOME=/home/ernest/hadoop-3.2.0
export HADOOP_MAPRED_HOME=/home/ernest/hadoop-3.2.0
export PATH=$PATH:$HADOOP_COMMON_HOME/bin
export PATH=$PATH:$HADOOP_COMMON_HOME/sbin

save file then Run below command if it comes on next cmd prompt then file editing is proper

source ~/.bashrc
cat .bashrc

to ernest Checking of java and hadoop

java -version
hadoop version

sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh

Add the following line at the end of hadoop-env.sh file
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/

to save the changes in the file, press ctrl and x together.
then press Y
then press Enter key

$HADOOP_HOME/bin/hadoop

To_copy_XML_file_to_input 
mkdir ~/input
cp $HADOOP_HOME/etc/hadoop/*.xml ~/input
__________________________________________________________________________________________
AIM- Install Hadoop in Pseudo Distributed Mode on Ubuntu

all steps same as above till extraction 

then edit bachrc file -- need to add below new parameters 

below parameters for prac1 only
export JAVA_HOME=/lib/jvm/java-11-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin
export HADOOP_HOME=/home/hadoop/hadoop-3.2.0
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME= $HADOOP_HOME
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_COMMON_HOME/bin
export PATH=$PATH:$HADOOP_COMMON_HOME/sbin

open hadoop-env.sh file  
sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh 

Add the following line at the end of .sh file
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
=================================================================================
3 core-site.xml
=================================================================================
sudo nano /home/ernest/hadoop-3.2.0/etc/hadoop/core-site.xml
<configuration>
<property>
	<name>hadoop.tmp.dir</name>
	<value>/home/ernest/tmpdata</value>
	<description>A base for other temporarary directories</description>
</property>

<property>
	<name>fs.default.name</name>
	<value>hdfs://localhost:9000</value>
	<description>The name of the default file system.</description>
</property>
</configuration>

=================================================================================
4 hdfs-site.xml
=================================================================================
sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml

<configuration>
<property>
	<name>dfs.data.dir</name>
	<value>/home/ ernest /dfsdata/namenode</value>
	<description>Location of namenode</description>
</property>

<property>
	<name>dfs.data.dir</name>
	<value>/home/ ernest /dfsdata/datanode</value>
	<description>Location of datanode</description>
</property>

<property>
	<name>dfs.replication</name>
	<value>1</value>
	<description>Replication Factor</description>
</property>
</configuration>

=================================================================================
5 mapred-site.xml
=================================================================================
sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
<configuration>

<property>
	<name>mapreduce.framework.name</name>
	<value>yarn</value>
	<description>Name of my mapreduce framework</description>
</property>
</configuration>

=================================================================================
6 yarn-site.xml
=================================================================================
sudo nano $HADOOP_HOME/etc/hadoop/yarn-site.xml

<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>

<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>

<property>
<name>yarn.resourcemanager.hostname</name>
<value>127.0.0.1</value>
</property>

<property>
<name>yarn.acl.enable</name>
<value>0</value>
</property>

<property>
<name>yarn.nodemanager.env-whitelist</name>
<value>JAVA_HOME, HADOOP_COMMON_HOME, HADOOP_HDFS, HADOOP_YARN</value>
</property>
</configuration>

===============================================================================
Format Namenode
=================================================================================
move to the /bin folder
cd /home/ernest/hadoop-3.2.0/bin

Now format the namenode using the following command
hdfs namenode -format

=================================================================================
Start the namenode, datanode
=================================================================================
move to the /sbin folder

cd /home/ernest/hadoop-3.2.0/sbin
Ls
start-dfs.sh
start-yarn.sh

=================================================================================
To check if Hadoop started correctly
=================================================================================
jps

=================================================================================
Copy file from local system to hdfs
=================================================================================
hdfs dfs -put /home/udit/download/employee_details.txt    -- source will be path that u will be copying

check using below listing command on terminal
hdfs dfs -ls

=================================================================================
Check with browser
=================================================================================
DFS overview - http://localhost:9870/ 
Datanode     -  http://localhost:9864/
